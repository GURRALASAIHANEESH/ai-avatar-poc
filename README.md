# Real-Time AI Avatar System (WebSocket + Wav2Lip + TTS)

This project is a real-time, lip-synced AI avatar system that integrates:

* ChatGPT for conversational responses
* Text-to-Speech (TTS) for voice synthesis
* Wav2Lip for accurate facial animation
* Web frontend for user interaction
* WebSocket backend for real-time streaming

## ğŸ”§ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ client_web/                     # Frontend (static web client)
â”‚   â”œâ”€â”€ index.html                  # Main UI with JavaScript & CSS
â”‚   â””â”€â”€ ...                         # Supporting JS, CSS, media assets
â”‚
â”œâ”€â”€ wav2lip_ws_server/             # Backend + ML pipeline (from Hugging Face)
â”‚   â””â”€â”€ gradio-lipsync-wav2lip/    # WebSocket + FastAPI backend
â”‚       â”œâ”€â”€ server.py              # WebSocket server and FastAPI app
â”‚       â”œâ”€â”€ inference.py           # Wav2Lip + TTS + processing logic
â”‚       â”œâ”€â”€ windows_config.py      # Windows-specific compatibility patches
â”‚       â”œâ”€â”€ audio.py               # Audio preprocessing and mel conversion
â”‚       â”œâ”€â”€ file_manager.py        # Upload and manage video/audio files
â”‚       â”œâ”€â”€ wav2lip_models/        # Wav2Lip model and checkpoint files
â”‚       â”œâ”€â”€ face_detection/        # S3FD-based face detector
â”‚       â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚       â”œâ”€â”€ .env.example           # Example env file
â”‚       â””â”€â”€ utils/                 # Utility modules (optional)
â”‚
â”œâ”€â”€ whisper.cpp/                  # (Optional) Offline STT module
â”‚   â”œâ”€â”€ main, models, etc.
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ’¡ Features

* ChatGPT integration for intelligent dialogue
* Text-to-Speech (TTS) pipeline
* Wav2Lip-based face animation
* Real-time WebSocket communication
* Simple web frontend for voice/text input
* Optional: Offline speech-to-text via `whisper.cpp`

## ğŸ“¦ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2. Install Python dependencies

```bash
cd wav2lip_ws_server/gradio-lipsync-wav2lip
pip install -r requirements.txt
```

### 3. Setup `.env`

Copy and fill in the environment config:

```bash
cp .env.example .env
```

### 4. Run Backend Server (WebSocket + FastAPI)

```bash
python server.py
```

The server will start on `ws://localhost:8001/ws/lipsync`

### 5. Open Frontend

Open `client_web/index.html` in your browser. This connects to the WebSocket backend and allows voice/text interaction.

## âœ… Windows-Specific Instructions

If you're on Windows, ensure `windows_config.py` is imported in `server.py` to patch system compatibility issues.

## ğŸ“ Key Files and Directories

| Path                    | Description                          |
| ----------------------- | ------------------------------------ |
| `server.py`             | FastAPI + WebSocket server           |
| `inference.py`          | TTS + Wav2Lip logic                  |
| `windows_config.py`     | Windows compatibility patch          |
| `file_manager.py`       | Handles input/output file management |
| `audio.py`              | Audio processing for mel conversion  |
| `face_detection/`       | S3FD-based face detection            |
| `wav2lip_models/`       | Model weights and inference code     |
| `client_web/index.html` | Web frontend interface               |

## ğŸ§ª Testing

* Input a message or voice from the browser.
* Backend streams TTS â†’ lip-sync video â†’ sends back final video.

## ğŸ—‚ï¸ Optional Whisper Integration

* Add `whisper.cpp/` to use offline STT. Currently not integrated in pipeline.

## ğŸ” .gitignore

Make sure the following are excluded:

```
*.pth
__pycache__/
*.mp4
*.wav
.env
```

## ğŸ”„ Acknowledgments
* Hugging Face Spaces (for base backend)
* Gradio (initial interface version)

---

For questions or improvements, feel free to raise an issue or pull request.
